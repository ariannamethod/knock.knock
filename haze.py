# haze.py — Reweight-GPT with Hybrid Attention (NumPy inference)
#
# Architecture:
#   - HybridHead = ReweightHead (positional) + ContentHead (semantic)
#   - Pre-norm blocks with GELU activation
#   - Entropy-aware adaptive temperature
#   - Multiple sampling strategies (top-p, top-k, mirostat)
#
# Can be randomly initialized OR loaded from .npz exported by train.py

from __future__ import annotations
import numpy as np
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Literal

from nn import (
    get_rng,
    init_weight,
    softmax,
    gelu,
    layer_norm,
    rms_norm,
    sample_basic,
    sample_top_k,
    sample_top_p,
    entropy_temperature,
    entropy_bits,
    confidence_score,
)


# ----------------- vocab -----------------


@dataclass
class Vocab:
    """Character-level vocabulary."""

    chars: List[str]
    stoi: dict
    itos: dict
    vocab_size: int

    @classmethod
    def from_text(cls, text: str) -> "Vocab":
        text = text.lower()
        chars = sorted(list(set(text)))
        stoi = {ch: i for i, ch in enumerate(chars)}
        itos = {i: ch for i, ch in enumerate(chars)}
        return cls(chars=chars, stoi=stoi, itos=itos, vocab_size=len(chars))

    def encode(self, s: str) -> List[int]:
        s = s.lower()
        return [self.stoi[c] for c in s if c in self.stoi]

    def decode(self, idxs: List[int]) -> str:
        return "".join(self.itos.get(i, "?") for i in idxs)


# ----------------- attention heads -----------------


class ReweightHead:
    """
    Reweight attention: learns positional attention patterns directly.
    Instead of QK^T, uses x @ W_reweight → (T, T) attention matrix.
    
    Captures: rhythm, n-gram patterns, positional dependencies.
    """

    def __init__(self, n_emb: int, head_dim: int, T: int, rng):
        self.wv = init_weight((n_emb, head_dim), rng=rng)
        self.wr = init_weight((n_emb, T), rng=rng)  # reweight projection
        self.T = T
        self.head_dim = head_dim

    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        x: (T, n_emb)
        returns: (T, head_dim)
        """
        v = x @ self.wv  # (T, head_dim)
        attn = x @ self.wr  # (T, T)

        # causal mask
        T = min(x.shape[0], self.T)
        tril = np.tril(np.ones((T, T), dtype=np.float32))
        mask = np.where(tril == 1.0, 0.0, -1e9)
        attn = attn[:T, :T] + mask

        rew = softmax(attn, axis=-1)  # (T, T)
        out = rew @ v[:T]  # (T, head_dim)
        return out


class ContentHead:
    """
    Content-based attention: classic QK^T / sqrt(d) attention.
    
    Captures: semantic similarity, long-range dependencies.
    """

    def __init__(self, n_emb: int, head_dim: int, T: int, rng):
        self.wq = init_weight((n_emb, head_dim), rng=rng)
        self.wk = init_weight((n_emb, head_dim), rng=rng)
        self.wv = init_weight((n_emb, head_dim), rng=rng)
        self.T = T
        self.head_dim = head_dim
        self.scale = 1.0 / np.sqrt(head_dim)

    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        x: (T, n_emb)
        returns: (T, head_dim)
        """
        q = x @ self.wq  # (T, head_dim)
        k = x @ self.wk  # (T, head_dim)
        v = x @ self.wv  # (T, head_dim)

        attn = (q @ k.T) * self.scale  # (T, T)

        # causal mask
        T = min(x.shape[0], self.T)
        tril = np.tril(np.ones((T, T), dtype=np.float32))
        mask = np.where(tril == 1.0, 0.0, -1e9)
        attn = attn[:T, :T] + mask

        attn = softmax(attn, axis=-1)
        out = attn @ v[:T]
        return out


class HybridHead:
    """
    Hybrid attention: combines Reweight (positional) + Content (semantic).
    
    The mix ratio α controls the blend:
        output = α * reweight_out + (1-α) * content_out
    
    This allows the model to use positional patterns (rhythm, structure)
    AND semantic similarity (meaning) simultaneously.
    """

    def __init__(
        self,
        n_emb: int,
        head_dim: int,
        T: int,
        rng,
        alpha: float = 0.5,  # reweight vs content mix
    ):
        self.reweight = ReweightHead(n_emb, head_dim, T, rng)
        self.content = ContentHead(n_emb, head_dim, T, rng)
        self.alpha = alpha
        self.head_dim = head_dim

        # learnable gate (initialized to alpha)
        self.gate = np.array([alpha], dtype=np.float32)

    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        x: (T, n_emb)
        returns: (T, head_dim)
        """
        r_out = self.reweight.forward(x)
        c_out = self.content.forward(x)

        # gated combination
        alpha = float(self.gate[0])
        return alpha * r_out + (1.0 - alpha) * c_out


# ----------------- block -----------------


class Block:
    """
    Transformer block with:
    - Pre-norm (more stable for deep networks)
    - Hybrid attention heads
    - GELU activation (smoother than ReLU)
    - Residual connections
    """

    def __init__(
        self,
        n_emb: int,
        T: int,
        nodes: int,
        rng,
        n_heads: int = 4,
        head_type: Literal["hybrid", "reweight", "content"] = "hybrid",
        alpha: float = 0.5,
    ):
        head_dim = n_emb // n_heads

        # create heads based on type
        if head_type == "hybrid":
            self.heads = [
                HybridHead(n_emb, head_dim, T, rng, alpha=alpha)
                for _ in range(n_heads)
            ]
        elif head_type == "reweight":
            self.heads = [
                ReweightHead(n_emb, head_dim, T, rng) for _ in range(n_heads)
            ]
        else:  # content
            self.heads = [
                ContentHead(n_emb, head_dim, T, rng) for _ in range(n_heads)
            ]

        # MLP
        self.w0 = init_weight((n_emb, nodes), rng=rng)
        self.w1 = init_weight((nodes, n_emb), rng=rng)

        # layer norm parameters
        self.ln1_gamma = np.ones(n_emb, dtype=np.float32)
        self.ln1_beta = np.zeros(n_emb, dtype=np.float32)
        self.ln2_gamma = np.ones(n_emb, dtype=np.float32)
        self.ln2_beta = np.zeros(n_emb, dtype=np.float32)

        self.n_emb = n_emb
        self.head_type = head_type

    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        x: (T, n_emb)
        returns: (T, n_emb)
        """
        # pre-norm attention
        x_norm = layer_norm(x, self.ln1_gamma, self.ln1_beta)
        h = [head.forward(x_norm) for head in self.heads]
        h = np.concatenate(h, axis=-1)  # (T, n_emb)
        x = x + h  # residual

        # pre-norm MLP
        x_norm = layer_norm(x, self.ln2_gamma, self.ln2_beta)
        h = x_norm @ self.w0
        h = gelu(h)
        h = h @ self.w1
        x = x + h  # residual

        return x


# ----------------- model -----------------


class ReweightGPT:
    """
    Reweight-GPT: character-level language model with hybrid attention.
    
    Features:
    - Hybrid heads (reweight + content attention)
    - Pre-norm blocks with GELU
    - Entropy-aware adaptive temperature
    - Multiple sampling strategies
    """

    def __init__(
        self,
        vocab_size: int,
        T: int = 16,
        n_emb: int = 32,
        nodes: int = 32,
        n_blocks: int = 3,
        n_heads: int = 4,
        head_type: Literal["hybrid", "reweight", "content"] = "hybrid",
        alpha: float = 0.5,
        seed: Optional[int] = 42,
    ):
        self.T = T
        self.n_emb = n_emb
        self.nodes = nodes
        self.n_blocks = n_blocks
        self.n_heads = n_heads
        self.head_type = head_type
        self.alpha = alpha
        self.vocab_size = vocab_size
        self.rng = get_rng(seed)

        # embeddings
        self.embed = init_weight((vocab_size, n_emb), rng=self.rng)
        self.pos = init_weight((T, n_emb), rng=self.rng)

        # blocks
        self.blocks = [
            Block(
                n_emb,
                T,
                nodes,
                rng=self.rng,
                n_heads=n_heads,
                head_type=head_type,
                alpha=alpha,
            )
            for _ in range(n_blocks)
        ]

        # final layer norm
        self.ln_f_gamma = np.ones(n_emb, dtype=np.float32)
        self.ln_f_beta = np.zeros(n_emb, dtype=np.float32)

        # output projection
        self.w2 = init_weight((n_emb, vocab_size), rng=self.rng)

    def logits(self, idx_seq: np.ndarray) -> np.ndarray:
        """
        Forward pass.
        
        idx_seq: (T,) int array of token indices
        returns: (T, vocab_size) logits
        """
        T = len(idx_seq)
        x = self.embed[idx_seq] + self.pos[:T]  # (T, n_emb)

        for block in self.blocks:
            x = block.forward(x)

        x = layer_norm(x, self.ln_f_gamma, self.ln_f_beta)
        logits = x @ self.w2  # (T, vocab_size)
        return logits

    def generate(
        self,
        seed_seq: List[int],
        length: int = 200,
        temperature: float = 1.0,
        sampling: Literal["basic", "top_k", "top_p", "entropy"] = "entropy",
        top_k: int = 40,
        top_p: float = 0.9,
        target_entropy: float = 3.0,
        min_temp: float = 0.3,
        max_temp: float = 2.0,
    ) -> tuple[List[int], dict]:
        """
        Generate tokens with various sampling strategies.
        
        Args:
            seed_seq: initial token indices
            length: number of tokens to generate
            temperature: base temperature (used differently per strategy)
            sampling: strategy - "basic", "top_k", "top_p", or "entropy"
            top_k: k for top-k sampling
            top_p: p for nucleus sampling
            target_entropy: target entropy for entropy-aware sampling
            min_temp, max_temp: bounds for adaptive temperature
        
        Returns:
            (tokens, stats) where stats contains generation metrics
        """
        T = self.T

        # prepare sequence
        if not seed_seq:
            seed_seq = [0]

        seq = list(seed_seq)
        if len(seq) < T:
            pad_val = seq[0]
            seq = [pad_val] * (T - len(seq)) + seq
        else:
            seq = seq[-T:]

        seq = np.array(seq, dtype=np.int32)
        out = []

        # stats tracking
        entropies = []
        confidences = []
        temps_used = []

        for _ in range(length):
            logits = self.logits(seq)
            logits_last = logits[-1]

            # track metrics
            probs = softmax(logits_last)
            entropies.append(entropy_bits(probs))
            confidences.append(confidence_score(logits_last))

            # sampling strategy
            if sampling == "entropy":
                # adaptive temperature based on current entropy
                temp = entropy_temperature(
                    logits_last,
                    target_entropy=target_entropy,
                    min_temp=min_temp,
                    max_temp=max_temp,
                )
                temps_used.append(temp)
                nxt = sample_top_p(logits_last, top_p, temp, self.rng)

            elif sampling == "top_p":
                temps_used.append(temperature)
                nxt = sample_top_p(logits_last, top_p, temperature, self.rng)

            elif sampling == "top_k":
                temps_used.append(temperature)
                nxt = sample_top_k(logits_last, top_k, temperature, self.rng)

            else:  # basic
                temps_used.append(temperature)
                nxt = sample_basic(logits_last, temperature, self.rng)

            out.append(nxt)

            # shift window
            seq = np.roll(seq, -1)
            seq[-1] = nxt

        stats = {
            "mean_entropy": float(np.mean(entropies)),
            "mean_confidence": float(np.mean(confidences)),
            "mean_temp": float(np.mean(temps_used)),
            "min_entropy": float(np.min(entropies)),
            "max_entropy": float(np.max(entropies)),
            "entropy_std": float(np.std(entropies)),
        }

        return out, stats

    # ----- simple generate for compatibility -----

    def generate_simple(
        self,
        seed_seq: List[int],
        length: int = 200,
        temperature: float = 1.0,
    ) -> List[int]:
        """Simple generation without stats (for compatibility)."""
        tokens, _ = self.generate(
            seed_seq,
            length=length,
            temperature=temperature,
            sampling="basic",
        )
        return tokens

    # ----- weight loading from .npz -----

    @classmethod
    def from_npz(cls, vocab_size: int, path: str | Path) -> "ReweightGPT":
        """
        Load weights from .npz file exported by train.py.
        
        Note: This loads as reweight-only heads (no content heads)
        to match the training architecture. Use head_type="reweight"
        or retrain with hybrid heads for full hybrid inference.
        """
        path = Path(path)
        data = np.load(path, allow_pickle=False)

        T = int(data["T"])
        n_emb = int(data["n_emb"])
        nodes = int(data["nodes"])
        n_blocks = int(data["n_blocks"])
        n_heads = int(data["n_heads"])
        saved_vocab_size = int(data["vocab_size"])

        if saved_vocab_size != vocab_size:
            raise ValueError(
                f"Vocab size mismatch: npz={saved_vocab_size}, current={vocab_size}"
            )

        model = cls(
            vocab_size=vocab_size,
            T=T,
            n_emb=n_emb,
            nodes=nodes,
            n_blocks=n_blocks,
            n_heads=n_heads,
            head_type="reweight",  # trained model uses reweight heads
            seed=None,
        )

        # top-level
        model.embed = data["embed"].astype("float32")
        model.pos = data["pos"].astype("float32")
        model.w2 = data["w2"].astype("float32")

        # blocks / heads
        for b in range(n_blocks):
            block = model.blocks[b]
            block.w0 = data[f"blocks.{b}.w0"].astype("float32")
            block.w1 = data[f"blocks.{b}.w1"].astype("float32")

            for h in range(n_heads):
                head = block.heads[h]
                head.wv = data[f"blocks.{b}.heads.{h}.wv"].astype("float32")
                head.wr = data[f"blocks.{b}.heads.{h}.wr"].astype("float32")

        return model


# ----------------- helpers -----------------


def load_corpus(path: str | Path) -> str:
    """Load text corpus from file."""
    path = Path(path)
    with path.open("r", encoding="utf-8") as f:
        return f.read()


def build_model_from_text(
    path: str | Path,
    T: int = 16,
    n_emb: int = 32,
    nodes: int = 32,
    n_blocks: int = 3,
    n_heads: int = 4,
    head_type: Literal["hybrid", "reweight", "content"] = "hybrid",
    alpha: float = 0.5,
    seed: Optional[int] = 42,
):
    """Build model and vocab from text file."""
    text = load_corpus(path)
    vocab = Vocab.from_text(text)
    model = ReweightGPT(
        vocab_size=vocab.vocab_size,
        T=T,
        n_emb=n_emb,
        nodes=nodes,
        n_blocks=n_blocks,
        n_heads=n_heads,
        head_type=head_type,
        alpha=alpha,
        seed=seed,
    )
    return text, vocab, model
